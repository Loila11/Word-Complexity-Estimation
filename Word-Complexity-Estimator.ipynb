{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "lines = loadtxt('train_full.txt', dtype='str', delimiter='\\t')\n",
    "\n",
    "labels = ['id', 'sentence', 'start', 'end', 'target', 'native', 'non-native', 'native_score', 'non-native_score', 'label']\n",
    "\n",
    "data = [dict(zip(labels, line)) for line in lines]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_targets = [line for line in data if len(line['target'].split()) == 1]\n",
    "len(word_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def makeTable(features, scores, frequency, file):\n",
    "    features = sorted(set([feature for feature in features if feature]))\n",
    "    scores = list(sorted(set(scores)))\n",
    "\n",
    "    info = [scores]\n",
    "    for feature in features:\n",
    "        line = []\n",
    "        for score in scores:\n",
    "            if (score, feature) not in frequency.keys():\n",
    "                frequency[(score, feature)] = 0\n",
    "            line.append(freq[(score, feature)])\n",
    "        info.append(line)\n",
    "\n",
    "    table = tabulate(info, headers='firstrow', showindex=features)\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(target):\n",
    "    scores = []\n",
    "\n",
    "    for line in target:\n",
    "        scores.append(line['label'])\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = getScores(word_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreq(features, scores, target):\n",
    "    freq = {}\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for line in target:\n",
    "        score = scores[i]\n",
    "        feature = features[j]\n",
    "\n",
    "        if (score, feature) not in freq.keys():\n",
    "            freq[(score, feature)] = 0\n",
    "\n",
    "        freq[(score, feature)] = freq[(score, feature)] + 1\n",
    "\n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLength(target):\n",
    "    freq = {}\n",
    "    lengths = []\n",
    "\n",
    "    for line in target:\n",
    "        lengths.append(len(line['target']))\n",
    "    return lengths\n",
    "\n",
    "lengths = getLength(word_targets)\n",
    "freq = getFreq(lengths, scores, word_targets)\n",
    "makeTable(lengths, scores, freq, 'statistics/lengths-score-freq.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp_stanza = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_stanza(word_targets[0]['sentence'])\n",
    "print(*[f'word: {word.text}\\tdeprel: {word.deprel}\\n' for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDep(line):\n",
    "    doc = nlp_stanza(line['sentence'])\n",
    "    pos = 0\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if pos >= int(line['start']):\n",
    "                return word.deprel\n",
    "            pos += len(word.text) + 1\n",
    "\n",
    "dep = []\n",
    "for line in word_targets[:18]:\n",
    "    dep.append(getDep(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordID(start, sentence):\n",
    "    pos = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i >= start:\n",
    "            return pos\n",
    "        if sentence[i] == ' ':\n",
    "            pos += 1\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def getPOS(line):\n",
    "    pos = getWordID(int(line['start']), line['sentence'])\n",
    "    text = pos_tag(word_tokenize(line['sentence']))\n",
    "    for i in range(len(text)):\n",
    "        if i >= pos and text[i][0] == line['target']:\n",
    "            return text[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for line in word_targets:\n",
    "    pos.append(getPOS(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = getFreq(pos, scores, word_targets)\n",
    "makeTable(pos, scores, freq, 'statistics/pos-score-freq.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "\n",
    "lines = loadtxt('train_full.txt', dtype='str', delimiter='\\t')\n",
    "linesTest = loadtxt('test.txt', dtype='str', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainData():\n",
    "    return [line[4].lower() for line in lines]\n",
    "\n",
    "\n",
    "def getTrainLabel():\n",
    "    return [line[9] for line in lines]\n",
    "\n",
    "\n",
    "def getTestData():\n",
    "    return [line[4].lower() for line in linesTest]\n",
    "\n",
    "\n",
    "def getTestLabel():\n",
    "    return [line[10] for line in linesTest]\n",
    "\n",
    "\n",
    "def getDataFrame():\n",
    "    train_df = pd.DataFrame(getTrainData(), columns = ['data'])\n",
    "    train_df['label'] = getTrainLabel()\n",
    "\n",
    "    test_df = pd.DataFrame(getTestData(), columns = ['data'])\n",
    "#     test_df['label'] = getTestLabel()\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def splitTrainTest():\n",
    "    data_df = pd.DataFrame(getTrainData(), columns = ['data'])\n",
    "    data_df['label'] = getTrainLabel()\n",
    "\n",
    "    return train_test_split(data_df, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Vectorizer():\n",
    "    vector_size = 300\n",
    "\n",
    "    def __init__(self, model, train_df, test_df):\n",
    "        self.model = model\n",
    "\n",
    "        if model == 'tfidf':\n",
    "            self.makeDatasetFromWords()\n",
    "\n",
    "            self.vectorizer = TfidfVectorizer(lowercase = False)\n",
    "            self.vectorizer.fit(self.dataset)\n",
    "\n",
    "        elif model == 'word2vec':\n",
    "            self.makeDatasetFromSentences()\n",
    "\n",
    "            self.vectorizer = Word2Vec(sentences=self.dataset, vector_size=300, window=5, min_count=1, workers=16)\n",
    "            self.vectorizer.save(\"word2vec.model\")\n",
    "\n",
    "        elif model == 'semantic':\n",
    "            self.makeDatasetFromWords()\n",
    "\n",
    "\n",
    "        self.vectorize(train_df, test_df)\n",
    "\n",
    "\n",
    "    def makeDatasetFromSentences(self):\n",
    "        brownData = [' '.join(sentence).lower() for sentence in brown.sents()]\n",
    "        self.dataset = np.array(list(set(\n",
    "            [line[1].lower() for line in lines] + [line[1].lower() for line in linesTest] + brownData\\\n",
    "        )))\n",
    "        self.preprocessSentences()\n",
    "\n",
    "\n",
    "    def makeDatasetFromWords(self):\n",
    "        self.dataset = np.array(list(set(\n",
    "            [line[1].lower() for line in lines] + [line[1].lower() for line in linesTest]\\\n",
    "        )))\n",
    "        self.preprocessWords()\n",
    "\n",
    "\n",
    "    def preprocessWords(self):\n",
    "        self.dataset = np.array(np.hstack([self.tokenizePhrase(phrase) for phrase in self.dataset]))\n",
    "\n",
    "\n",
    "    def preprocessSentences(self):\n",
    "        tokens = set([' '.join(self.tokenizePhrase(phrase)) for phrase in self.dataset])\n",
    "        self.dataset = np.array([tokensList.split() for tokensList in tokens])\n",
    "\n",
    "\n",
    "    def tokenizePhrase(self, phrase):\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z\\'-]+')\n",
    "        return word_tokenize(' '.join(tokenizer.tokenize(phrase)))\n",
    "\n",
    "\n",
    "    def vectorize(self, train_df, test_df):\n",
    "        if self.model == 'tfidf':\n",
    "            self.X_train = self.vectorizer.transform(train_df['data']).toarray()\n",
    "            self.X_test = self.vectorizer.transform(test_df['data']).toarray()\n",
    "\n",
    "\n",
    "        elif self.model == 'word2vec':\n",
    "            self.X_train = np.array([self.vectorizePhrase(phrase, self.vectorizer.wv.get_vector) for phrase in train_df['data']])\n",
    "            self.X_test = np.array([self.vectorizePhrase(phrase, self.vectorizer.wv.get_vector) for phrase in test_df['data']])\n",
    "            \n",
    "        elif self.model == 'semantic':\n",
    "            self.X_train = np.array([self.vectorizePhrase(phrase, self.extractFeatures) for phrase in train_df['data']])\n",
    "            self.X_test = np.array([self.vectorizePhrase(phrase, self.extractFeatures) for phrase in test_df['data']])\n",
    "            \n",
    "        self.y_train = np.array(train_df['label'], dtype='float')\n",
    "#         self.y_test = np.array(test_df['label'], dtype='float')\n",
    "\n",
    "\n",
    "    def vectorizePhrase(self, phrase, func):\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z\\'-]+')\n",
    "        vectorizations = np.array([func(word) for word in self.tokenizePhrase(phrase)])\n",
    "\n",
    "        return np.mean(vectorizations, axis=0)\n",
    "\n",
    "\n",
    "    def extractFeatures(self, word):\n",
    "        vowels = sum(map(word.count, 'aeiou'))\n",
    "        length = len(word)\n",
    "        doubleLetters = sum([word[i - 1] == word[i] for i in range(1, length)])\n",
    "        \n",
    "        maxConsecutiveConsonants = 0\n",
    "        localSum = 0\n",
    "        for l in word:\n",
    "            if l not in 'aeiou':\n",
    "                maxConsecutiveConsonants = max(maxConsecutiveConsonants, localSum)\n",
    "                localSum = 0\n",
    "            else:\n",
    "                localSum += 1\n",
    "        maxConsecutiveConsonants = max(maxConsecutiveConsonants, localSum)\n",
    "\n",
    "        return [length, vowels / length, doubleLetters, maxConsecutiveConsonants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = getDataFrame()\n",
    "\n",
    "tfidf = Vectorizer('tfidf', train_df, test_df)\n",
    "w2v = Vectorizer('word2vec', train_df, test_df)\n",
    "features = Vectorizer('semantic', train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from matplotlib.pyplot import hist\n",
    "\n",
    "\n",
    "class Solve:\n",
    "    def __init__(self, vectorizer):\n",
    "        getDataFrame()\n",
    "#         self.splitTrainTest()\n",
    "\n",
    "        self.X_train = vectorizer.X_train\n",
    "        self.X_test = vectorizer.X_test\n",
    "\n",
    "        self.y_train = vectorizer.y_train\n",
    "#         self.y_test = vectorizer.y_test\n",
    "\n",
    "        self.X_train = np.c_[self.X_train, features.X_train]\n",
    "        self.X_test = np.c_[self.X_test, features.X_test]\n",
    "\n",
    "        self.y_train = self.y_train.reshape(-1, 1)\n",
    "#         self.y_test = self.y_test.reshape(-1, 1)\n",
    "\n",
    "#         self.getScores()\n",
    "#         self.findScore()\n",
    "\n",
    "\n",
    "    def scaleData(self, y_scaler):\n",
    "        X_scaler = StandardScaler()\n",
    "        X_train = X_scaler.fit_transform(self.X_train)\n",
    "        X_test = X_scaler.transform(self.X_test)\n",
    "\n",
    "        y_train = y_scaler.fit_transform(self.y_train)\n",
    "#         y_test = y_scaler.transform(self.y_test)\n",
    "\n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "    # - Validation --------------------------------------------------\n",
    "\n",
    "    def validation(self, model, to_scale):\n",
    "        if to_scale:\n",
    "            y_scaler = StandardScaler()\n",
    "            X_train, X_test, y_train, y_test = self.scaleData(y_scaler)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            return self.scaled_mae(y_train, model.predict(X_train), y_scaler), \\\n",
    "                self.scaled_mae(y_test, model.predict(X_test), y_scaler)\n",
    "\n",
    "        else:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return mean_absolute_error(self.y_train, model.predict(self.X_train)), \\\n",
    "                mean_absolute_error(self.y_test, model.predict(self.X_test))\n",
    "\n",
    "\n",
    "    def scaled_mae(self, y_true, y_pred, scaler):\n",
    "        return mean_absolute_error(scaler.inverse_transform(y_true), scaler.inverse_transform(y_pred))\n",
    "\n",
    "    \n",
    "    def printScore(self, model, model_name, to_scale = True):\n",
    "        train_score, test_score = self.validation(model, to_scale)\n",
    "\n",
    "        print(f'{model_name}: \\n' + \\\n",
    "             f'Train MAE: {train_score} \\n' + \\\n",
    "             f'Test MAE: {test_score} \\n')\n",
    "\n",
    "\n",
    "    # - Train -------------------------------------------------------\n",
    "\n",
    "    def train(self, model, to_scale):\n",
    "        if to_scale:\n",
    "            y_scaler = StandardScaler()\n",
    "            X_train, X_test, y_train = self.scaleData(y_scaler)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            self.res = y_scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "        else:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            self.res = model.predict(self.X_test)\n",
    "\n",
    "        return np.c_[[line[0] for line in linesTest], self.res]\n",
    "\n",
    "\n",
    "    def saveScore(self, file, solution):\n",
    "        np.savetxt('./submissions/' + file, solution, delimiter=',', header='id,label', comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_w2v = Solve(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = Solve(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sol_w2v_knn_scaling = solution_w2v.train(KNeighborsRegressor(n_neighbors=2), True)\n",
    "sol_w2v_knn = solution_w2v.train(KNeighborsRegressor(n_neighbors=2), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_w2v_lr = solution_w2v.train(LogisticRegression(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_w2v_nusvr = solution_w2v.train(NuSVR(C=10, coef0=1.0), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_w2v_lsvr = solution_w2v.train(LinearSVR(C=5), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_w2v_ridge = solution_w2v.train(Ridge(alpha=1.0), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_w2v_rfr = solution_w2v.train(RandomForestRegressor(n_estimators=1000, max_depth=300, random_state=0), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_w2v.saveScore('submission5.csv', sol_w2v_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
