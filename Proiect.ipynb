{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "lines = loadtxt('train_full.txt', dtype='str', delimiter='\\t')\n",
    "\n",
    "labels = ['id', 'sentence', 'start', 'end', 'target', 'native', 'non-native', 'native_score', 'non-native_score', 'label']\n",
    "\n",
    "data = [dict(zip(labels, line)) for line in lines]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_targets = [line for line in data if len(line['target'].split()) == 1]\n",
    "len(word_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeTable(features, scores, frequency, file):\n",
    "    features = sorted(set([feature for feature in features if feature]))\n",
    "    scores = list(sorted(set(scores)))\n",
    "\n",
    "    info = [scores]\n",
    "    for feature in features:\n",
    "        line = []\n",
    "        for score in scores:\n",
    "            if (score, feature) not in frequency.keys():\n",
    "                frequency[(score, feature)] = 0\n",
    "            line.append(freq[(score, feature)])\n",
    "        info.append(line)\n",
    "\n",
    "    table = tabulate(info, headers='firstrow', showindex=features)\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(target):\n",
    "    scores = []\n",
    "\n",
    "    for line in target:\n",
    "        scores.append(line['label'])\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = getScores(word_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreq(features, scores, target):\n",
    "    freq = {}\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for line in target:\n",
    "        score = scores[i]\n",
    "        feature = features[j]\n",
    "\n",
    "        if (score, feature) not in freq.keys():\n",
    "            freq[(score, feature)] = 0\n",
    "\n",
    "        freq[(score, feature)] = freq[(score, feature)] + 1\n",
    "\n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLength(target):\n",
    "    freq = {}\n",
    "    lengths = []\n",
    "\n",
    "    for line in target:\n",
    "        lengths.append(len(line['target']))\n",
    "    return lengths\n",
    "\n",
    "lengths = getLength(word_targets)\n",
    "freq = getFreq(lengths, scores, word_targets)\n",
    "makeTable(lengths, scores, freq, 'lengths-score-freq.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp_stanza = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_stanza(word_targets[0]['sentence'])\n",
    "print(*[f'word: {word.text}\\tdeprel: {word.deprel}\\n' for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDep(line):\n",
    "    doc = nlp_stanza(line['sentence'])\n",
    "    pos = 0\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if pos >= int(line['start']):\n",
    "                return word.deprel\n",
    "            pos += len(word.text) + 1\n",
    "\n",
    "dep = []\n",
    "for line in word_targets[:18]:\n",
    "    dep.append(getDep(line))\n",
    "#     print(f\"{line['target']} {getDep(line)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordID(start, sentence):\n",
    "    pos = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i >= start:\n",
    "            return pos\n",
    "        if sentence[i] == ' ':\n",
    "            pos += 1\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def getPOS(line):\n",
    "    pos = getWordID(int(line['start']), line['sentence'])\n",
    "    text = pos_tag(word_tokenize(line['sentence']))\n",
    "    for i in range(len(text)):\n",
    "        if i >= pos and text[i][0] == line['target']:\n",
    "            return text[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for line in word_targets:\n",
    "    pos.append(getPOS(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = getFreq(pos, scores, word_targets)\n",
    "makeTable(pos, scores, freq, 'pos-score-freq.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(w2v_model.wv.most_similar(positive='chinese'))\n",
    "print(w2v_model.wv.most_similar(positive='family'))\n",
    "print(w2v_model.wv.most_similar(positive='country'))\n",
    "print(w2v_model.wv.most_similar(positive='attack'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_word_lines = [line[:5] for line in lines if len(line[4].split()) == 1]\n",
    "one_word_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\mirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "lines = loadtxt('train_full.txt', dtype='str', delimiter='\\t')\n",
    "linesTest = loadtxt('test.txt', dtype='str', delimiter='\\t')\n",
    "# linesTest = loadtxt('News_Dev.tsv', dtype='str', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainData():\n",
    "    return [line[4].lower() for line in lines]\n",
    "\n",
    "\n",
    "def getTrainLabel():\n",
    "    return [line[9] for line in lines]\n",
    "\n",
    "\n",
    "def getTestData():\n",
    "    return [line[4].lower() for line in linesTest]\n",
    "\n",
    "\n",
    "def getTestLabel():\n",
    "    return [line[10] for line in linesTest]\n",
    "\n",
    "\n",
    "def getDataFrame():\n",
    "    train_df = pd.DataFrame(getTrainData(), columns = ['data'])\n",
    "    train_df['label'] = getTrainLabel()\n",
    "\n",
    "    test_df = pd.DataFrame(getTestData(), columns = ['data'])\n",
    "#     test_df['label'] = getTestLabel()\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def splitTrainTest():\n",
    "    data_df = pd.DataFrame(getTrainData(), columns = ['data'])\n",
    "    data_df['label'] = getTrainLabel()\n",
    "\n",
    "    return train_test_split(data_df, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class Vectorizer():\n",
    "    vector_size = 300\n",
    "\n",
    "    def __init__(self, model, train_df, test_df):\n",
    "        self.model = model\n",
    "\n",
    "        if model == 'tfidf':\n",
    "            self.makeDatasetFromWords()\n",
    "\n",
    "            self.vectorizer = TfidfVectorizer(lowercase = False)\n",
    "            self.vectorizer.fit(self.dataset)\n",
    "\n",
    "        elif model == 'word2vec':\n",
    "            self.makeDatasetFromSentences()\n",
    "\n",
    "            self.vectorizer = Word2Vec(sentences=self.dataset, vector_size=300, window=5, min_count=1, workers=16)\n",
    "            self.vectorizer.save(\"word2vec.model\")\n",
    "\n",
    "        elif model == 'fastText':\n",
    "            self.makeDatasetFromSentences()\n",
    "            \n",
    "            self.vectorizer = FastText(vector_size=300, window=5, min_count=1)\n",
    "            self.vectorizer.build_vocab(corpus_iterable=self.dataset)\n",
    "            self.vectorizer.train(corpus_iterable=self.dataset, total_examples=len(self.dataset), epochs=10)\n",
    "\n",
    "        elif model == 'semantic':\n",
    "            self.makeDatasetFromWords()\n",
    "\n",
    "\n",
    "        self.vectorize(train_df, test_df)\n",
    "\n",
    "\n",
    "#     def makeDatasetFromSentences(self):\n",
    "#         self.dataset = np.array(list(set([line[1].lower() for line in lines] + [line[1].lower() for line in linesTest])))\n",
    "#         self.preprocessSentences()\n",
    "\n",
    "    def makeDatasetFromSentences(self):\n",
    "        brownData = [' '.join(sentence).lower() for sentence in brown.sents()]\n",
    "        self.dataset = np.array(list(set([line[1].lower() for line in lines] + [line[1].lower() for line in linesTest] + brownData)))\n",
    "        self.preprocessSentences()\n",
    "\n",
    "\n",
    "    def makeDatasetFromWords(self):\n",
    "        self.dataset = np.array(list(set(\n",
    "            [line[1].lower() for line in lines] + [line[1].lower() for line in linesTest] + \\\n",
    "            [line[4].lower() for line in lines] + [line[4].lower() for line in linesTest] \\\n",
    "        )))\n",
    "        self.preprocessWords()\n",
    "\n",
    "\n",
    "    def preprocessWords(self):\n",
    "        self.dataset = np.array(list(set(np.hstack([self.tokenizePhrase(phrase) for phrase in self.dataset]))))\n",
    "\n",
    "\n",
    "    def preprocessSentences(self):\n",
    "        tokens = set([' '.join(self.tokenizePhrase(phrase)) for phrase in self.dataset])\n",
    "        self.dataset = np.array([tokensList.split() for tokensList in tokens])\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         tokenizer = RegexpTokenizer(r'[a-zA-Z\\'-]+')\n",
    "#         self.dataset = np.array(list(set(np.hstack([self.tokenizePhrase(phrase) for phrase in self.dataset]))))\n",
    "\n",
    "\n",
    "    def tokenizePhrase(self, phrase):\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z\\'-]+')\n",
    "        return word_tokenize(' '.join(tokenizer.tokenize(phrase)))\n",
    "\n",
    "\n",
    "    def vectorize(self, train_df, test_df):\n",
    "        if self.model == 'tfidf':\n",
    "            self.X_train = self.vectorizer.transform(train_df['data']).toarray()\n",
    "            self.X_test = self.vectorizer.transform(test_df['data']).toarray()\n",
    "\n",
    "\n",
    "        elif self.model in ['word2vec', 'fastText']:\n",
    "            self.X_train = np.array([self.vectorizePhrase(phrase, self.vectorizer.wv.get_vector) for phrase in train_df['data']])\n",
    "            self.X_test = np.array([self.vectorizePhrase(phrase, self.vectorizer.wv.get_vector) for phrase in test_df['data']])\n",
    "            \n",
    "        elif self.model == 'semantic':\n",
    "            self.X_train = np.array([self.vectorizePhrase(phrase, self.extractFeatures) for phrase in train_df['data']])\n",
    "            self.X_test = np.array([self.vectorizePhrase(phrase, self.extractFeatures) for phrase in test_df['data']])\n",
    "            \n",
    "        self.y_train = np.array(train_df['label'], dtype='float')\n",
    "#         self.y_test = np.array(test_df['label'], dtype='float')\n",
    "\n",
    "\n",
    "    def vectorizePhrase(self, phrase, func):\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z\\'-]+')\n",
    "        vectorizations = np.array([func(word) for word in self.tokenizePhrase(phrase)])\n",
    "\n",
    "        return np.mean(vectorizations, axis=0)\n",
    "\n",
    "\n",
    "    def extractFeatures(self, word):\n",
    "        vowels = sum(map(word.count, 'aeiou'))\n",
    "        length = len(word)\n",
    "        doubleLetters = sum([word[i - 1] == word[i] for i in range(1, length)])\n",
    "        \n",
    "        maxConsecutiveConsonants = 0\n",
    "        localSum = 0\n",
    "        for l in word:\n",
    "            if l not in 'aeiou':\n",
    "                maxConsecutiveConsonants = max(maxConsecutiveConsonants, localSum)\n",
    "                localSum = 0\n",
    "            else:\n",
    "                localSum += 1\n",
    "        maxConsecutiveConsonants = max(maxConsecutiveConsonants, localSum)\n",
    "        \n",
    "#         senses = len(wordnet.synsets(word))\n",
    "\n",
    "        return [length, vowels / length, doubleLetters, maxConsecutiveConsonants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = getDataFrame()\n",
    "# tfidf = Vectorizer('tfidf', train_df, test_df)\n",
    "w2v = Vectorizer('word2vec', train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Vectorizer('semantic', train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from matplotlib.pyplot import hist\n",
    "\n",
    "\n",
    "class Solve:\n",
    "    def __init__(self, vectorizer):\n",
    "        getDataFrame()\n",
    "#         self.splitTrainTest()\n",
    "\n",
    "        self.X_train = vectorizer.X_train\n",
    "        self.X_test = vectorizer.X_test\n",
    "\n",
    "        self.y_train = vectorizer.y_train\n",
    "#         self.y_test = vectorizer.y_test\n",
    "\n",
    "        self.X_train = np.c_[self.X_train, features.X_train]\n",
    "        self.X_test = np.c_[self.X_test, features.X_test]\n",
    "\n",
    "        self.y_train = self.y_train.reshape(-1, 1)\n",
    "#         self.y_test = self.y_test.reshape(-1, 1)\n",
    "\n",
    "#         self.getScores()\n",
    "        self.findScore()\n",
    "\n",
    "\n",
    "    def scaleData(self, y_scaler):\n",
    "        X_scaler = StandardScaler()\n",
    "        X_train = X_scaler.fit_transform(self.X_train)\n",
    "        X_test = X_scaler.transform(self.X_test)\n",
    "\n",
    "        y_train = y_scaler.fit_transform(self.y_train)\n",
    "#         y_test = y_scaler.transform(self.y_test)\n",
    "\n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "    # - Train --------------------------------------------------\n",
    "\n",
    "    def train(self, model, to_scale):\n",
    "        if to_scale:\n",
    "            y_scaler = StandardScaler()\n",
    "            X_train, X_test, y_train, y_test = self.scaleData(y_scaler)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            return self.scaled_mae(y_train, model.predict(X_train), y_scaler), \\\n",
    "                self.scaled_mae(y_test, model.predict(X_test), y_scaler)\n",
    "\n",
    "        else:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return mean_absolute_error(self.y_train, model.predict(self.X_train)), \\\n",
    "                mean_absolute_error(self.y_test, model.predict(self.X_test))\n",
    "\n",
    "\n",
    "    def scaled_mae(self, y_true, y_pred, scaler):\n",
    "        return mean_absolute_error(scaler.inverse_transform(y_true), scaler.inverse_transform(y_pred))\n",
    "\n",
    "    \n",
    "    def printScore(self, model, model_name, to_scale = True):\n",
    "        train_score, test_score = self.train(model, to_scale)\n",
    "\n",
    "        print(f'{model_name}: \\n' + \\\n",
    "             f'Train MAE: {train_score} \\n' + \\\n",
    "             f'Test MAE: {test_score} \\n')\n",
    "\n",
    "\n",
    "    def getScores(self):\n",
    "#         self.printScore(LogisticRegression(), \"Logistic Regression\", False)\n",
    "#         self.printScore(NuSVR(C=10, coef0=1.0), \"Nu-SVR\")\n",
    "#         self.printScore(NuSVR(C=10, coef0=1.0), \"Nu-SVR - no scaling\", False)\n",
    "#         self.printScore(LinearSVR(C=5), \"Linear SVR\")\n",
    "#         self.printScore(LinearSVR(C=5), \"Linear SVR - no scaling\", False)\n",
    "#         self.printScore(Ridge(alpha=1.0), \"Ridge\")\n",
    "#         self.printScore(Ridge(alpha=1.0), \"Ridge - no scaling\", False)\n",
    "        self.printScore(KNeighborsRegressor(n_neighbors=2), f\"kNN + features\")\n",
    "        self.printScore(KNeighborsRegressor(n_neighbors=2), f\"kNN + features - no scaling\", False)\n",
    "#         self.printScore(RandomForestRegressor(n_estimators=1000, max_depth=300, random_state=0), \"Forest - no scaling\", False)\n",
    "#         self.printScore(RandomForestRegressor(n_estimators=1000, max_depth=300, random_state=0), \"Forest\")\n",
    "\n",
    "\n",
    "    def findScore(self):\n",
    "        y_scaler = StandardScaler()\n",
    "        X_train, X_test, y_train = self.scaleData(y_scaler)\n",
    "        \n",
    "        model = KNeighborsRegressor(n_neighbors=2)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        self.res = y_scaler.inverse_transform(model.predict(X_test))\n",
    "        np.savetxt('./submissions/submission4.csv', np.c_[[line[0] for line in linesTest], self.res], delimiter=',', header='id,label', comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = Solve(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN: \n",
      "Train MAE: 0.04981788315954863 \n",
      "Test MAE: 0.07103174603174603 \n",
      "\n",
      "kNN - no scaling: \n",
      "Train MAE: 0.049177498452602005 \n",
      "Test MAE: 0.07015306122448979 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "solutions = Solve(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN + features: \n",
      "Train MAE: 0.05073203828024568 \n",
      "Test MAE: 0.07240173847316705 \n",
      "\n",
      "kNN + features - no scaling: \n",
      "Train MAE: 0.04570180450411847 \n",
      "Test MAE: 0.06779100529100529 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "solutions = Solve(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'barren' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0cfea4dbcb06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msolutions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-7af399b6c5fd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#         features_test = self.getFeatures(getTestData())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-73e7d8e1f30b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, train_df, test_df)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-73e7d8e1f30b>\u001b[0m in \u001b[0;36mvectorize\u001b[1;34m(self, train_df, test_df)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word2vec'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fastText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-73e7d8e1f30b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word2vec'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fastText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-73e7d8e1f30b>\u001b[0m in \u001b[0;36mvectorizePhrase\u001b[1;34m(self, phrase, func)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[a-zA-Z\\'-]+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mvectorizations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-73e7d8e1f30b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvectorizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[a-zA-Z\\'-]+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mvectorizations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizePhrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'barren' not present\""
     ]
    }
   ],
   "source": [
    "solutions = Solve('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVR: \n",
      "Train MAE: 0.08793877766907492 \n",
      "Test MAE: 0.08937679791441001 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVR - no scaling: \n",
      "Train MAE: 0.14775185495883286 \n",
      "Test MAE: 0.14767033132083182 \n",
      "\n",
      "Ridge: \n",
      "Train MAE: 0.10599724255115797 \n",
      "Test MAE: 0.10616548577217227 \n",
      "\n",
      "Ridge - no scaling: \n",
      "Train MAE: 0.10599765861633148 \n",
      "Test MAE: 0.10616541260915204 \n",
      "\n",
      "kNN: \n",
      "Train MAE: 0.0815719182973861 \n",
      "Test MAE: 0.09128684807256236 \n",
      "\n",
      "kNN - no scaling: \n",
      "Train MAE: 0.07958005999142981 \n",
      "Test MAE: 0.08863945578231293 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "solutions = Solve('semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Nu-SVR: \n",
      "Train MAE: 0.040018643439954746 \n",
      "Test MAE: 0.06639279337937004 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Nu-SVR - no scaling: \n",
      "Train MAE: 0.021147066881803674 \n",
      "Test MAE: 0.06102471514487684 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "solutions = Solve('tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
